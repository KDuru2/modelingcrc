{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f2a7bc78-98fb-4001-b909-af3281c73376",
   "metadata": {},
   "source": [
    "## Data Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0b39e510-64e2-421e-87c8-94d05a15a1d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch as T\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_selection import SelectPercentile, mutual_info_classif, RFECV\n",
    "from sklearn.metrics import precision_recall_curve, f1_score, accuracy_score, precision_score, recall_score, roc_curve, auc, roc_auc_score\n",
    "from sklearn import preprocessing\n",
    "import matplotlib.pyplot as plt\n",
    "import shap\n",
    "import ray\n",
    "from ray import tune\n",
    "from ray.tune.schedulers import ASHAScheduler\n",
    "from ray.tune.search.hyperopt import HyperOptSearch\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.svm import SVC\n",
    "from hyperopt import hp\n",
    "import xgboost as xgb\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "08c0b722-fc29-4f8a-82cc-725107ad1ebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cancer gene list initialization\n",
    "cancerdf = pd.read_csv(\"/gpfs/data/dgamsiz/kduru1/data/known_cancer.txt\", sep =\"\\t\")\n",
    "cancer_genes = list(cancerdf['Symbol'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "06b49aa5-b7fe-4409-a69e-afb6fd211a30",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_997638/1576194340.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  rnadf['avg_' + duplicate] = avg\n",
      "/tmp/ipykernel_997638/1576194340.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  rnadf['avg_' + duplicate] = avg\n",
      "/tmp/ipykernel_997638/1576194340.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  rnadf['avg_' + duplicate] = avg\n",
      "/tmp/ipykernel_997638/1576194340.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  rnadf['avg_' + duplicate] = avg\n",
      "/tmp/ipykernel_997638/1576194340.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  rnadf['avg_' + duplicate] = avg\n"
     ]
    }
   ],
   "source": [
    "#RNA Dataframe initialization\n",
    "rnadf = pd.read_csv(\"/gpfs/data/dgamsiz/kduru1/data/modified_rnaseq_data.txt\", sep =\"\\t\")\n",
    "\n",
    "rnadf = rnadf.transpose()\n",
    "rnadf.drop('Entrez_Gene_Id',inplace=True)\n",
    "rnadf = rnadf.reset_index() \n",
    "rnadf['index'] = rnadf['index'].str.replace('-01', '')\n",
    "rnadf.columns = rnadf.iloc[0]\n",
    "rnadf = rnadf.drop(0)\n",
    "rnadf = rnadf.rename(columns={'Hugo_Symbol': 'PATIENT_ID'})\n",
    "rnadf.columns = rnadf.columns.astype(str)\n",
    "rnadf.set_index('PATIENT_ID', inplace = True)\n",
    "#Removes missing RNA data\n",
    "rnadf.dropna(axis=1, inplace=True)\n",
    "\n",
    "#Averages duplicate RNA data\n",
    "duplicates = rnadf.columns[rnadf.columns.duplicated()]\n",
    "\n",
    "for duplicate in duplicates:\n",
    "    avg = rnadf[duplicate].mean(axis=1)\n",
    "    rnadf['avg_' + duplicate] = avg\n",
    "\n",
    "rnadf.drop(duplicates, axis=1, inplace=True)\n",
    "\n",
    "rnadf = rnadf.add_prefix('rna_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5bdbcac2-dbd0-44fd-b981-ed908705ac5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_997638/100718712.py:2: DtypeWarning: Columns (4,38,39) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  mutdf = pd.read_csv(\"/gpfs/data/dgamsiz/kduru1/data/data_mutations.txt\", sep =\"\\t\")\n"
     ]
    }
   ],
   "source": [
    "#Mutation DataFrame Initialization\n",
    "mutdf = pd.read_csv(\"/gpfs/data/dgamsiz/kduru1/data/data_mutations.txt\", sep =\"\\t\")\n",
    "\n",
    "columns_to_keep = ['Hugo_Symbol', 'Variant_Classification', 'Tumor_Sample_Barcode']\n",
    "mutdf = mutdf[columns_to_keep]\n",
    "mutdf = mutdf.set_index('Tumor_Sample_Barcode')\n",
    "\n",
    "mutdf = mutdf[~mutdf['Variant_Classification'].isin([\"Silent\",\"3'Flank\",\"3'UTR\",\"5'Flank\",\"5'UTR\",\"Intron\"])]\n",
    "mutdf.drop('Variant_Classification', axis=1, inplace=True)\n",
    "mutdf.index = [s.strip('-01') for s in mutdf.index]\n",
    "mutdf.index.names = ['PATIENT_ID']\n",
    "mutdf = pd.get_dummies(mutdf, columns=['Hugo_Symbol'])\n",
    "\n",
    "mutdf = mutdf.groupby(\"PATIENT_ID\").max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ec77ce1a-56a7-4bc4-9977-609e319ba0da",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_997638/3717578574.py:25: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  statusdf.replace(to_replace='Progressive Disease', value=False, inplace=True)\n",
      "/tmp/ipykernel_997638/3717578574.py:27: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  clindf.replace(to_replace='Female', value= 0, inplace=True)\n"
     ]
    }
   ],
   "source": [
    "#Initialize Clinical Dataframe and combine with Treatment Dataframe\n",
    "clindf = pd.read_csv(\"/gpfs/data/dgamsiz/kduru1/data/data_clinical_patient.txt\", sep =\"\\t\")\n",
    "treatmentdf = pd.read_csv(\"/gpfs/data/dgamsiz/kduru1/data/data_timeline_treatment.txt\", sep =\"\\t\")\n",
    "statdf = pd.read_csv(\"/gpfs/data/dgamsiz/kduru1/data/data_timeline_status.txt\", sep =\"\\t\")\n",
    "\n",
    "clindf.set_index('PATIENT_ID', inplace=True)\n",
    "statdf.set_index('PATIENT_ID', inplace=True)\n",
    "statusdf = statdf['PRIMARY_THERAPY_OUTCOME_SUCCESS'].dropna()\n",
    "\n",
    "\n",
    "for col in treatmentdf.columns:\n",
    "    if col != 'AGENT' and col != 'MEASURE_OF_RESPONSE' and col != 'PATIENT_ID' and col != 'START_DATE':\n",
    "        treatmentdf.drop(col, axis=1, inplace=True)\n",
    "\n",
    "#Transform data into binary format - 0.5\n",
    "treatmentdf.replace(to_replace='clinical progressive disease', value=False, inplace=True)\n",
    "treatmentdf.replace(to_replace='stable progressive disease', value=False, inplace=True)\n",
    "treatmentdf.replace(to_replace='radiographic progressive disease', value=False, inplace=True)\n",
    "treatmentdf.replace(to_replace='stable disease', value=False, inplace=True)\n",
    "treatmentdf.replace(to_replace='partial response', value=True, inplace=True)\n",
    "treatmentdf.replace(to_replace='complete response', value=True, inplace=True)\n",
    "statusdf.replace(to_replace='Complete Remission/Response', value=True, inplace=True)\n",
    "statusdf.replace(to_replace='Partial Remission/Response', value=True, inplace=True)\n",
    "statusdf.replace(to_replace='Stable Disease', value=False, inplace=True)\n",
    "statusdf.replace(to_replace='Progressive Disease', value=False, inplace=True)\n",
    "clindf.replace(to_replace='Male', value= 1, inplace=True)\n",
    "clindf.replace(to_replace='Female', value= 0, inplace=True)\n",
    "\n",
    "#Merge treatmentdf with clin params of interest\n",
    "paramsetup = {\n",
    "    'SEX' : clindf['SEX'],\n",
    "    'ANCES' : clindf['GENETIC_ANCESTRY_LABEL'],\n",
    "    'AGE' : clindf['AGE']\n",
    "}\n",
    "\n",
    "clinparameters = pd.DataFrame(paramsetup)\n",
    "idparamdf = treatmentdf.merge(clinparameters, on='PATIENT_ID')\n",
    "idparamdf.set_index('PATIENT_ID', inplace=True)\n",
    "\n",
    "\n",
    "#Augments treatment database with overall status database\n",
    "nans = idparamdf['MEASURE_OF_RESPONSE'].isna()\n",
    "for pt in idparamdf.index:\n",
    "    if nans[pt].all() and pt in statusdf.index:\n",
    "        last_med = np.max(idparamdf.loc[pt,'START_DATE'])\n",
    "        idparamdf.loc[(idparamdf['START_DATE'] == last_med) & (idparamdf.index == pt), 'MEASURE_OF_RESPONSE'] = statusdf[pt]\n",
    "\n",
    "idparamdf.replace(' ', np.nan, inplace=True)\n",
    "idparamdf.dropna(axis=0, inplace=True)\n",
    "\n",
    "#Remove pts missing clinical data\n",
    "idparamdf.replace(' ', np.nan, inplace=True)\n",
    "idparamdf.dropna(axis=0, inplace=True)\n",
    "\n",
    "#One-hot encoding for categorical data\n",
    "idparamdf = pd.get_dummies(idparamdf, columns=['AGENT'])\n",
    "\n",
    "\n",
    "non_med = ['AGENT_Radiation 1', 'AGENT_Nos', 'AGENT_Radiation 2']\n",
    "\n",
    "#if non_med in idparamdf.columns:\n",
    "idparamdf.drop(non_med, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "517a973a-0b77-421b-a32e-4aacb5c34091",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_997638/4005035102.py:39: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  clindf.iloc[row][addagent] = True\n"
     ]
    }
   ],
   "source": [
    "#Combines concurrent regiments into one entry based on start date\n",
    "agents = list()\n",
    "agents.append('MEASURE_OF_RESPONSE')\n",
    "\n",
    "for item in list(idparamdf.columns):\n",
    "    if 'AGENT' in item:\n",
    "        agents.append(item)\n",
    "\n",
    "clindf = pd.DataFrame()\n",
    "\n",
    "for pt in idparamdf.index.unique():\n",
    "    df = idparamdf.loc[pt]\n",
    "    starts = np.sort(np.unique(df['START_DATE']))\n",
    "    if isinstance(df, pd.DataFrame):\n",
    "        startdf = df[(df['START_DATE'] == starts[0])]\n",
    "        startdf = startdf[agents]\n",
    "        boolsum = startdf.apply(lambda row: any(row), axis=0)\n",
    "        new_row = pd.DataFrame(boolsum).transpose()\n",
    "        new_row.index = [pt]\n",
    "        clindf = pd.concat([clindf, new_row])\n",
    "    else:\n",
    "        df = df[agents]\n",
    "        new_row = pd.DataFrame(df).transpose()\n",
    "        clindf = pd.concat([clindf, new_row])\n",
    "\n",
    "\n",
    "agents.remove('MEASURE_OF_RESPONSE')\n",
    "valid_single_agents = [x.replace('AGENT_','') for x in agents if \"+\" not in x]\n",
    "double_agents = [x for x in agents if \"+\" in x]\n",
    "\n",
    "#Handle '+' medication entries\n",
    "for row in range(len(clindf)):\n",
    "    for col in clindf.columns:\n",
    "        if '+' in col and clindf.iloc[row][col]:\n",
    "            single_agents = col.replace('AGENT_','').split(' + ')\n",
    "            for agent in single_agents:\n",
    "                if agent in valid_single_agents:\n",
    "                    addagent = 'AGENT_' + agent\n",
    "                    clindf.iloc[row][addagent] = True\n",
    "\n",
    "clindf.drop(columns=double_agents, inplace=True)\n",
    "\n",
    "#Add back in ages, sex, MOR, ancestry\n",
    "ages = idparamdf['AGE'].loc[~idparamdf['AGE'].index.duplicated(keep='first')]\n",
    "sex = idparamdf['SEX'].loc[~idparamdf['SEX'].index.duplicated(keep='first')]\n",
    "ances = idparamdf['ANCES'].loc[~idparamdf['ANCES'].index.duplicated(keep='first')]\n",
    "\n",
    "clindf = clindf.join(ages)\n",
    "clindf = clindf.join(sex)\n",
    "clindf = clindf.join(ances)\n",
    "\n",
    "clindf = pd.get_dummies(clindf, columns=['ANCES'])\n",
    "clindf.index.name = 'PATIENT_ID'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b1053220-c578-4d32-8234-8d4350acd933",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Merge dfs\n",
    "halfmergedf = clindf.join(mutdf, how=\"inner\")\n",
    "mergedf = halfmergedf.join(rnadf, how=\"inner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "41acd3e9-7c5e-4d10-8a65-9353b8a88034",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Shuffles dataframe with reproducible seed\n",
    "ids = mergedf.index.unique().tolist()\n",
    "random.seed(23)\n",
    "random.shuffle(ids)\n",
    "shuffledf = mergedf.reset_index()\n",
    "shuffledf = shuffledf.set_index('PATIENT_ID').loc[ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c72063ff-858d-48ca-9da0-8f93f8c9722b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_997638/1931087092.py:5: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  nonconstantdf.replace(to_replace=False, value=0 , inplace=True)\n",
      "/tmp/ipykernel_997638/1931087092.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  nonconstantdf.replace(to_replace=False, value=0 , inplace=True)\n",
      "/tmp/ipykernel_997638/1931087092.py:6: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  nonconstantdf.replace(to_replace=True, value=1, inplace = True)\n",
      "/tmp/ipykernel_997638/1931087092.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  nonconstantdf.replace(to_replace=True, value=1, inplace = True)\n"
     ]
    }
   ],
   "source": [
    "#Drop constant values\n",
    "nonconstantdf = shuffledf.loc[:, shuffledf.nunique() != 1]\n",
    "\n",
    "#Convert logical values to ints\n",
    "nonconstantdf.replace(to_replace=False, value=0 , inplace=True)\n",
    "nonconstantdf.replace(to_replace=True, value=1, inplace = True)\n",
    "nonconstantdf= nonconstantdf.astype(float)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f2c3960-bf4d-4813-ae2e-a83abdca43b7",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "36db5600-3ed2-483e-b272-e955adb98d3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split into train (70%) and test (30%) with stratification\n",
    "train_df, test_df = train_test_split(nonconstantdf, test_size=0.3, random_state=41, stratify=nonconstantdf['MEASURE_OF_RESPONSE'])\n",
    "\n",
    "# Split test into test (80%) and validation (20%) with stratification\n",
    "test_df, valid_df = train_test_split(test_df, test_size=0.2, random_state=41, stratify=test_df['MEASURE_OF_RESPONSE'])\n",
    "\n",
    "# Define target (y) and features (X)\n",
    "y_train = train_df.iloc[:, 0]\n",
    "df_train = train_df.iloc[:, 1:]\n",
    "\n",
    "y_test = test_df.iloc[:, 0]\n",
    "df_test = test_df.iloc[:, 1:]\n",
    "\n",
    "y_valid = valid_df.iloc[:, 0]\n",
    "df_valid = valid_df.iloc[:, 1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f726e3af-78fc-49d3-b4c4-1606730ffa21",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data Splitting\n",
    "rnadf_train = df_train.filter(like='rna_')\n",
    "mutdf_train = df_train.filter(like='Hugo_')\n",
    "clindf_train = df_train.drop(columns=rnadf_train.columns.union(mutdf_train.columns))\n",
    "\n",
    "rnadf_test = df_test.filter(like='rna_')\n",
    "mutdf_test = df_test.filter(like='Hugo_')\n",
    "clindf_test = df_test.drop(columns=rnadf_test.columns.union(mutdf_test.columns))\n",
    "\n",
    "rnadf_valid = df_valid.filter(like='rna_')\n",
    "mutdf_valid = df_valid.filter(like='Hugo_')\n",
    "clindf_valid = df_valid.drop(columns=rnadf_valid.columns.union(mutdf_valid.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3d5da80f-f6c4-44b6-aa3a-13abd729b5f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We adjudicated 866 features based on known cancer and 59334 features by variance and 0 features randomly.\n",
      "We removed 3232 total features.\n"
     ]
    }
   ],
   "source": [
    "#RNA Normalization\n",
    "scaler = preprocessing.Normalizer()\n",
    "scaler.fit(rnadf_train)\n",
    "\n",
    "srnadf_train = pd.DataFrame(scaler.transform(rnadf_train), \n",
    "                        columns=rnadf_train.columns, index=rnadf_train.index)\n",
    "\n",
    "srnadf_test = pd.DataFrame(scaler.transform(rnadf_test), \n",
    "                        columns=rnadf_test.columns, index=rnadf_test.index)\n",
    "\n",
    "srnadf_valid = pd.DataFrame(scaler.transform(rnadf_valid), \n",
    "                        columns=rnadf_valid.columns, index=rnadf_valid.index)\n",
    "\n",
    "srna_np = rnadf_train.to_numpy()\n",
    "corr_matrix = np.corrcoef(srna_np, rowvar=False)\n",
    "corr_triangle = np.triu(corr_matrix, k = 1)\n",
    "feature_var = rnadf_train.var()\n",
    "\n",
    "#Find associated features and remove\n",
    "to_remove = set()\n",
    "corr_threshold = 0.80\n",
    "c = 0\n",
    "v = 0\n",
    "\n",
    "corr_ind1, corr_ind2 = np.where(corr_triangle >= corr_threshold)\n",
    "\n",
    "#Which feature is known for cancer? Which feature has higher variance?\n",
    "for index in range(len(corr_ind1)):\n",
    "    corr_feature1 = srnadf_train.columns[corr_ind1[index]]\n",
    "    corr_feature2 = srnadf_train.columns[corr_ind2[index]]\n",
    "\n",
    "    if corr_feature1.strip('rna_') in cancer_genes and corr_feature2.strip('rna_') not in cancer_genes:\n",
    "        to_remove.add(corr_feature1)\n",
    "        c =  c+1\n",
    "\n",
    "    elif corr_feature2.strip('rna_') in cancer_genes and corr_feature1.strip('rna_') not in cancer_genes:\n",
    "        to_remove.add(corr_feature2)\n",
    "        c= c+1\n",
    "\n",
    "    elif feature_var[corr_feature1] > feature_var[corr_feature2]:\n",
    "        to_remove.add(corr_feature2)\n",
    "        v= v+1\n",
    "\n",
    "    elif feature_var[corr_feature2] > feature_var[corr_feature1]:\n",
    "        to_remove.add(corr_feature1)\n",
    "        v = v+1\n",
    "\n",
    "\n",
    "urnadf_train = rnadf_train.drop(columns = to_remove)\n",
    "urnadf_test = rnadf_test.drop(columns = to_remove)\n",
    "urnadf_valid = rnadf_valid.drop(columns = to_remove)\n",
    "\n",
    "\n",
    "print(f\"We adjudicated {c} features based on known cancer and {v} features by variance.\")\n",
    "print(f\"We removed {len(to_remove)} total features.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7eec9dc2-56da-475f-8235-4b5ae64218aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "preselector = SelectPercentile(mutual_info_classif, percentile=2)\n",
    "preselector.fit(urnadf_train, y_train)\n",
    "\n",
    "selected_features = preselector.get_support(indices=True) \n",
    "psrnadf_train = urnadf_train.iloc[:, selected_features]\n",
    "psrnadf_test = urnadf_test.iloc[:, selected_features]\n",
    "psrnadf_valid = urnadf_valid.iloc[:, selected_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ddeb15cf-6143-48be-92a3-2b2de5ba9afa",
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc = RandomForestClassifier(n_estimators=50, random_state=26, n_jobs=-1)\n",
    "selector = RFECV(rfc, min_features_to_select=200, step=5, cv=5, n_jobs=-1)\n",
    "selector.fit(psrnadf_train, y_train)\n",
    "\n",
    "selected_features = selector.get_support(indices=True) \n",
    "rfrnadf_train = psrnadf_train.iloc[:, selected_features]\n",
    "rfrnadf_test = psrnadf_test.iloc[:, selected_features]\n",
    "rfrnadf_valid = psrnadf_valid.iloc[:, selected_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e9529d83-8919-451c-9173-964a39009df0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Mutation Data Processing\n",
    "constant_cols = mutdf_train.loc[:, mutdf_train.nunique() == 1]\n",
    "\n",
    "vmutdf_train = mutdf_train.drop(columns=constant_cols)\n",
    "vmutdf_test = mutdf_test.drop(columns=constant_cols)\n",
    "vmutdf_valid = mutdf_valid.drop(columns=constant_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e2e63a56-1fd2-413d-ba6e-a2c7b4b8e00f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We adjudicated 118767 features based on known cancer and 183881 features by variance and 0 features randomly.\n",
      "We removed 3408 total features.\n"
     ]
    }
   ],
   "source": [
    "mut_np = vmutdf_train.to_numpy()\n",
    "corr_matrix = np.corrcoef(mut_np, rowvar=False)\n",
    "corr_triangle = np.triu(corr_matrix, k = 1)\n",
    "feature_var = vmutdf_train.var()\n",
    "\n",
    "#Find associated features and remove\n",
    "to_remove = set()\n",
    "corr_threshold = 0.80\n",
    "c = 0\n",
    "v = 0\n",
    "\n",
    "corr_ind1, corr_ind2 = np.where(corr_triangle >= corr_threshold)\n",
    "\n",
    "#Which feature is known for cancer? Which feature has higher variance?\n",
    "for index in range(len(corr_ind1)):\n",
    "    corr_feature1 = vmutdf_train.columns[corr_ind1[index]]\n",
    "    corr_feature2 = vmutdf_train.columns[corr_ind2[index]]\n",
    "\n",
    "    if corr_feature1.strip('Hugo_Symbol_') in cancer_genes and corr_feature2.strip('Hugo_Symbol_') not in cancer_genes:\n",
    "        to_remove.add(corr_feature1)\n",
    "        c =  c+1\n",
    "\n",
    "    elif corr_feature2.strip('Hugo_Symbol_') in cancer_genes and corr_feature1.strip('Hugo_Symbol_') not in cancer_genes:\n",
    "        to_remove.add(corr_feature2)\n",
    "        c= c+1\n",
    "\n",
    "    elif feature_var[corr_feature1] > feature_var[corr_feature2]:\n",
    "        to_remove.add(corr_feature2)\n",
    "        v= v+1\n",
    "\n",
    "    elif feature_var[corr_feature2] > feature_var[corr_feature1]:\n",
    "        to_remove.add(corr_feature1)\n",
    "        v = v+1\n",
    "\n",
    "\n",
    "umutdf_train = vmutdf_train.drop(columns = to_remove)\n",
    "umutdf_test = vmutdf_test.drop(columns = to_remove)\n",
    "umutdf_valid = vmutdf_valid.drop(columns = to_remove)\n",
    "\n",
    "\n",
    "print(f\"We adjudicated {c} features based on known cancer and {v} features by variance.\")\n",
    "print(f\"We removed {len(to_remove)} total features.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c738d72c-02ba-4a58-88c8-83dfd6aac1a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "preselector = SelectPercentile(mutual_info_classif, percentile=3)\n",
    "preselector.fit(umutdf_train, y_train)\n",
    "\n",
    "selected_features = preselector.get_support(indices=True) \n",
    "psmutdf_train = umutdf_train.iloc[:, selected_features]\n",
    "psmutdf_test = umutdf_test.iloc[:, selected_features]\n",
    "psmutdf_valid = umutdf_valid.iloc[:, selected_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "303b1b0e-15fc-48c7-bda0-e92f1b957429",
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc = RandomForestClassifier(n_estimators=50, random_state=26, n_jobs=-1)\n",
    "selector = RFECV(rfc, min_features_to_select=200, step=5, cv=5, n_jobs=-1)\n",
    "selector.fit(psmutdf_train, y_train)\n",
    "\n",
    "selected_features = selector.get_support(indices=True) \n",
    "rfmutdf_train = psmutdf_train.iloc[:, selected_features]\n",
    "rfmutdf_test = psmutdf_test.iloc[:, selected_features]\n",
    "rfmutdf_valid = psmutdf_valid.iloc[:, selected_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc391caa-a0a8-422d-92f1-8d7d278488a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Recombine categorical and numeric data\n",
    "mclindf_train = clindf_train.reset_index().drop('PATIENT_ID', axis = 1)\n",
    "mrnadf_train = rfrnadf_train.reset_index().drop('PATIENT_ID', axis = 1)\n",
    "mmutdf_train = rfmutdf_train.reset_index().drop('PATIENT_ID', axis = 1)\n",
    "halfmergedf_train = mclindf_train.join(mrnadf_train, how=\"inner\")\n",
    "mergedf_train = halfmergedf_train.join(mmutdf_train, how=\"inner\")\n",
    "\n",
    "mclindf_test = clindf_test.reset_index().drop('PATIENT_ID', axis = 1)\n",
    "mrnadf_test = rfrnadf_test.reset_index().drop('PATIENT_ID', axis = 1)\n",
    "mmutdf_test = rfmutdf_test.reset_index().drop('PATIENT_ID', axis = 1)\n",
    "halfmergedf_test = mclindf_test.join(mrnadf_test, how=\"inner\")\n",
    "mergedf_test = halfmergedf_test.join(mmutdf_test, how=\"inner\")\n",
    "\n",
    "mclindf_valid = clindf_valid.reset_index().drop('PATIENT_ID', axis = 1)\n",
    "mrnadf_valid = rfrnadf_valid.reset_index().drop('PATIENT_ID', axis = 1)\n",
    "mmutdf_valid = rfmutdf_valid.reset_index().drop('PATIENT_ID', axis = 1)\n",
    "halfmergedf_valid = mclindf_valid.join(mrnadf_valid, how=\"inner\")\n",
    "mergedf_valid = halfmergedf_valid.join(mmutdf_valid, how=\"inner\")\n",
    "\n",
    "mergedf_train.to_csv('<PATHNAME>', index=False)\n",
    "mergedf_test.to_csv('<PATHNAME>', index=False)\n",
    "mergedf_valid.to_csv('<PATHNAME>', index=False)\n",
    "y_train.to_csv('<PATHNAME>', index=False)\n",
    "y_test.to_csv('<PATHNAME>', index=False)\n",
    "y_valid.to_csv('<PATHNAME>', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd6f04b4-077e-4844-b84f-425110cc9140",
   "metadata": {},
   "source": [
    "## Final Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a65eaed6-875b-49d0-bf0a-d7937b257bc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_weights(m):\n",
    "    for layer in m.children():\n",
    "        if hasattr(layer, 'reset_parameters'):\n",
    "            layer.reset_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9ce56775-219a-4b8d-8f79-3909aca228b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loads saved data\n",
    "trainXdf = pd.read_csv('nfinaltrainX.csv')\n",
    "testXdf = pd.read_csv('nfinaltestX.csv')\n",
    "validXdf = pd.read_csv('nfinalvalidX.csv')\n",
    "trainYdf = pd.read_csv('ntrainY.csv')\n",
    "testYdf = pd.read_csv('ntestY.csv')\n",
    "validYdf = pd.read_csv('nvalidY.csv')\n",
    "\n",
    "#Creates tensors\n",
    "trainX = trainXdf.to_numpy()\n",
    "testX = testXdf.to_numpy()\n",
    "validX = validXdf.to_numpy()\n",
    "\n",
    "trainY = trainYdf.to_numpy()\n",
    "testY = testYdf.to_numpy()\n",
    "validY = validYdf.to_numpy()\n",
    "\n",
    "TrainX = T.tensor(trainX, dtype=T.float32)\n",
    "TrainY = T.tensor(trainY, dtype=T.float32).reshape(-1, 1)\n",
    "\n",
    "TestX = T.tensor(testX, dtype=T.float32)\n",
    "TestY = T.tensor(testY, dtype=T.float32).reshape(-1, 1)\n",
    "\n",
    "ValidX = T.tensor(validX, dtype=T.float32)\n",
    "ValidY = T.tensor(validY, dtype=T.float32).reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "179523d2-6e49-465b-9d4e-a87e92ac7db5",
   "metadata": {},
   "outputs": [],
   "source": [
    "paramcount = len(TestX[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "86ef4ac0-a274-480a-96b6-725dcfb662cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "nr = sum(trainY == 0)\n",
    "r = sum(trainY == 1)\n",
    "bal = nr/r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6b8bd65f-2307-4fab-ab57-960e7a222917",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_997638/1135591135.py:1: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:278.)\n",
      "  weight = T.tensor([bal], dtype=T.float32)\n"
     ]
    }
   ],
   "source": [
    "weight = T.tensor([bal], dtype=T.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a728132d-fb14-4ac1-b749-23d444d68680",
   "metadata": {},
   "outputs": [],
   "source": [
    "hparams = {\n",
    "    \"lr\" : hp.uniform('lr', 0.0001, 0.0009),\n",
    "    \"batch\": hp.randint('batch',15,30),\n",
    "    \"decay\" : hp.uniform('decay',0.001, 0.009),\n",
    "    \"do\" : hp.choice('do',[0.5,0.6,0.7, 0.8, 0.9]),\n",
    "    \n",
    "}\n",
    "\n",
    "hyperopt_search = HyperOptSearch(hparams, metric=\"loss\", mode=\"min\")\n",
    "\n",
    "def createNet(do):\n",
    "    model = nn.Sequential(\n",
    "    nn.Linear(paramcount, 1024),\n",
    "    nn.LeakyReLU(),\n",
    "    nn.Dropout(do),\n",
    "    nn.Linear(1024, 512),\n",
    "    nn.LeakyReLU(),\n",
    "    nn.Dropout(do),\n",
    "    nn.Linear(512, 256),\n",
    "    nn.LeakyReLU(),\n",
    "    nn.Linear(256,1)\n",
    ")\n",
    "    return model\n",
    "\n",
    "def train(hparams):\n",
    "    model = createNet(hparams[\"do\"])\n",
    "    reset_weights(model)\n",
    "    lossfxn = nn.BCEWithLogitsLoss(pos_weight = weight)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=hparams[\"lr\"], weight_decay = hparams[\"decay\"])\n",
    "\n",
    "    numepochs = 30\n",
    "    batchsize = hparams[\"batch\"]\n",
    "    for epoch in range(numepochs):\n",
    "        for i in range (0, len(TrainX), batchsize):\n",
    "            Xbatch = TrainX[i:i+batchsize]\n",
    "            ybatch = TrainY[i:i+batchsize]\n",
    "            ypredicted = model(Xbatch)\n",
    "            loss = lossfxn(ypredicted, ybatch)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # Calculate average validation loss\n",
    "        model.eval() \n",
    "        val_loss = 0.0\n",
    "        with T.no_grad():\n",
    "            y_prob = model(TestX).numpy()\n",
    "            for i in range(0, len(validX), batchsize):\n",
    "                Xbatch_val = ValidX[i:i+batchsize]\n",
    "                ybatch_val = ValidY[i:i+batchsize]\n",
    "                ypredicted_val = model(Xbatch_val)\n",
    "                val_loss_batch = lossfxn(ypredicted_val, ybatch_val)\n",
    "                val_loss += val_loss_batch.item()\n",
    "        val_loss /= (len(ValidX) / batchsize)\n",
    "        \n",
    "        # Calculate FPR, TPR, and thresholds\n",
    "        fpr, tpr, thresholds = roc_curve(TestY, y_prob)\n",
    "    \n",
    "        # Calculate the AUC (Area Under the Curve)\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "    \n",
    "        model.train()\n",
    "\n",
    "        ray.train.report(\n",
    "            {\"loss\" : val_loss,\n",
    "            \"AUC\" : roc_auc})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7e641bbe-454d-4290-b87b-62a7021c78b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"tuneStatus\">\n",
       "  <div style=\"display: flex;flex-direction: row\">\n",
       "    <div style=\"display: flex;flex-direction: column;\">\n",
       "      <h3>Tune Status</h3>\n",
       "      <table>\n",
       "<tbody>\n",
       "<tr><td>Current time:</td><td>2025-04-16 01:34:51</td></tr>\n",
       "<tr><td>Running for: </td><td>00:00:21.39        </td></tr>\n",
       "<tr><td>Memory:      </td><td>55.6/375.5 GiB     </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "    </div>\n",
       "    <div class=\"vDivider\"></div>\n",
       "    <div class=\"systemInfo\">\n",
       "      <h3>System Info</h3>\n",
       "      Using AsyncHyperBand: num_stopped=3<br>Bracket: Iter 20.000: -1.1705154946872167 | Iter 10.000: -1.3411672030176436<br>Logical resource usage: 10.0/32 CPUs, 0/1 GPUs (0.0/1.0 accelerator_type:RTX)\n",
       "    </div>\n",
       "    \n",
       "  </div>\n",
       "  <div class=\"hDivider\"></div>\n",
       "  <div class=\"trialStatus\">\n",
       "    <h3>Trial Status</h3>\n",
       "    <table>\n",
       "<thead>\n",
       "<tr><th>Trial name    </th><th>status    </th><th>loc                 </th><th style=\"text-align: right;\">  batch</th><th style=\"text-align: right;\">     decay</th><th style=\"text-align: right;\">  do</th><th style=\"text-align: right;\">         lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    loss</th><th style=\"text-align: right;\">     AUC</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>train_04c55282</td><td>PENDING   </td><td>                    </td><td style=\"text-align: right;\">     24</td><td style=\"text-align: right;\">0.00733347</td><td style=\"text-align: right;\"> 0.9</td><td style=\"text-align: right;\">0.000696433</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">        </td><td style=\"text-align: right;\">        </td></tr>\n",
       "<tr><td>train_519159d7</td><td>TERMINATED</td><td>172.20.216.7:3897442</td><td style=\"text-align: right;\">     25</td><td style=\"text-align: right;\">0.00377207</td><td style=\"text-align: right;\"> 0.5</td><td style=\"text-align: right;\">0.000622922</td><td style=\"text-align: right;\">    30</td><td style=\"text-align: right;\">         6.82183</td><td style=\"text-align: right;\">1.21271 </td><td style=\"text-align: right;\">0.714286</td></tr>\n",
       "<tr><td>train_ecfbbbf4</td><td>TERMINATED</td><td>172.20.216.7:3954512</td><td style=\"text-align: right;\">     26</td><td style=\"text-align: right;\">0.00756158</td><td style=\"text-align: right;\"> 0.8</td><td style=\"text-align: right;\">0.000599886</td><td style=\"text-align: right;\">    30</td><td style=\"text-align: right;\">         3.49858</td><td style=\"text-align: right;\">1.05859 </td><td style=\"text-align: right;\">0.705882</td></tr>\n",
       "<tr><td>train_2c4ec3df</td><td>TERMINATED</td><td>172.20.216.7:3998997</td><td style=\"text-align: right;\">     18</td><td style=\"text-align: right;\">0.0039877 </td><td style=\"text-align: right;\"> 0.7</td><td style=\"text-align: right;\">0.000718294</td><td style=\"text-align: right;\">    30</td><td style=\"text-align: right;\">         2.47073</td><td style=\"text-align: right;\">0.710554</td><td style=\"text-align: right;\">0.789916</td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "  </div>\n",
       "</div>\n",
       "<style>\n",
       ".tuneStatus {\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".tuneStatus .systemInfo {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       ".tuneStatus .trialStatus {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".tuneStatus .hDivider {\n",
       "  border-bottom-width: var(--jp-border-width);\n",
       "  border-bottom-color: var(--jp-border-color0);\n",
       "  border-bottom-style: solid;\n",
       "}\n",
       ".tuneStatus .vDivider {\n",
       "  border-left-width: var(--jp-border-width);\n",
       "  border-left-color: var(--jp-border-color0);\n",
       "  border-left-style: solid;\n",
       "  margin: 0.5em 1em 0.5em 1em;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(pid=3897442)\u001b[0m /oscar/home/kduru1/kanayo_env/lib64/python3.9/site-packages/torch/storage.py:414: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "\u001b[36m(pid=3897442)\u001b[0m   return torch.load(io.BytesIO(b))\n",
      "\u001b[36m(pid=3897442)\u001b[0m /oscar/home/kduru1/kanayo_env/lib64/python3.9/site-packages/torch/storage.py:414: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "\u001b[36m(pid=3897442)\u001b[0m   return torch.load(io.BytesIO(b))\n",
      "\u001b[36m(pid=3954512)\u001b[0m /oscar/home/kduru1/kanayo_env/lib64/python3.9/site-packages/torch/storage.py:414: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "\u001b[36m(pid=3954512)\u001b[0m   return torch.load(io.BytesIO(b))\n",
      "\u001b[36m(pid=3954512)\u001b[0m /oscar/home/kduru1/kanayo_env/lib64/python3.9/site-packages/torch/storage.py:414: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "\u001b[36m(pid=3954512)\u001b[0m   return torch.load(io.BytesIO(b))\n",
      "\u001b[36m(pid=3998997)\u001b[0m /oscar/home/kduru1/kanayo_env/lib64/python3.9/site-packages/torch/storage.py:414: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "\u001b[36m(pid=3998997)\u001b[0m   return torch.load(io.BytesIO(b))\n",
      "\u001b[36m(pid=3998997)\u001b[0m /oscar/home/kduru1/kanayo_env/lib64/python3.9/site-packages/torch/storage.py:414: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "\u001b[36m(pid=3998997)\u001b[0m   return torch.load(io.BytesIO(b))\n",
      "2025-04-16 01:34:51,743\tWARNING tune.py:219 -- Stop signal received (e.g. via SIGINT/Ctrl+C), ending Ray Tune run. This will try to checkpoint the experiment state one last time. Press CTRL+C (or send SIGINT/SIGKILL/SIGTERM) to skip. \n",
      "2025-04-16 01:34:51,780\tINFO tune.py:1009 -- Wrote the latest version of all result files and experiment state to '/users/kduru1/ray_results/train_2025-04-16_01-34-30' in 0.0363s.\n",
      "\u001b[36m(pid=4054209)\u001b[0m /oscar/home/kduru1/kanayo_env/lib64/python3.9/site-packages/torch/storage.py:414: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\u001b[32m [repeated 2x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/user-guides/configure-logging.html#log-deduplication for more options.)\u001b[0m\n",
      "\u001b[36m(pid=4054209)\u001b[0m   return torch.load(io.BytesIO(b))\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "2025-04-16 01:34:53,379\tINFO tune.py:1041 -- Total run time: 23.18 seconds (21.36 seconds for the tuning loop).\n",
      "2025-04-16 01:34:53,380\tWARNING tune.py:1056 -- Experiment has been interrupted, but the most recent state was saved.\n",
      "Resume experiment with: tune.run(..., resume=True)\n",
      "2025-04-16 01:34:53,393\tWARNING experiment_analysis.py:180 -- Failed to fetch metrics for 1 trial(s):\n",
      "- train_04c55282: FileNotFoundError('Could not fetch metrics for train_04c55282: both result.json and progress.csv were not found at /users/kduru1/ray_results/train_2025-04-16_01-34-30/train_04c55282_4_batch=24,decay=0.0073,do=0.9000,lr=0.0007_2025-04-16_01-34-47')\n"
     ]
    }
   ],
   "source": [
    "#Hyperparameter training\n",
    "analysis = tune.run(\n",
    "    train,\n",
    "    search_alg=hyperopt_search,\n",
    "    num_samples = 500,\n",
    "    max_concurrent_trials = 30,\n",
    "    scheduler=ASHAScheduler(\n",
    "        metric = \"loss\",\n",
    "        mode = \"min\",\n",
    "        max_t = 30,\n",
    "        grace_period = 10,\n",
    "        reduction_factor = 2),\n",
    "    resources_per_trial={\"cpu\": 10, \"gpu\": 0},\n",
    "    verbose = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e23a548-7676-4fa6-97d5-ceb2f31dd4ec",
   "metadata": {},
   "source": [
    "## Comparative Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "09ca3ae3-a3be-461d-9e34-3de93ddc0af1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loads saved data\n",
    "trainXdf = pd.read_csv('nfinaltrainX.csv')\n",
    "testXdf = pd.read_csv('nfinaltestX.csv')\n",
    "validXdf = pd.read_csv('nfinalvalidX.csv')\n",
    "trainYdf = pd.read_csv('ntrainY.csv')\n",
    "testYdf = pd.read_csv('ntestY.csv')\n",
    "validYdf = pd.read_csv('nvalidY.csv')\n",
    "\n",
    "#Creates tensors\n",
    "trainX = trainXdf.to_numpy()\n",
    "testX = testXdf.to_numpy()\n",
    "\n",
    "trainY = trainYdf.to_numpy()\n",
    "testY = testYdf.to_numpy()\n",
    "\n",
    "validX = validXdf.to_numpy()\n",
    "validY = validYdf.to_numpy()\n",
    "\n",
    "TrainX = T.tensor(trainX, dtype=T.float32)\n",
    "TrainY = T.tensor(trainY, dtype=T.float32).reshape(-1, 1)\n",
    "\n",
    "TestX = T.tensor(testX, dtype=T.float32)\n",
    "TestY = T.tensor(testY, dtype=T.float32).reshape(-1, 1)\n",
    "\n",
    "ValidX = T.tensor(validX, dtype=T.float32)\n",
    "ValidY = T.tensor(validY, dtype=T.float32).reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3bd5f4e3-6581-4875-81da-bdff1c19c7c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/oscar/home/kduru1/kanayo_env/lib64/python3.9/site-packages/sklearn/utils/validation.py:1408: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/oscar/home/kduru1/kanayo_env/lib64/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:1124: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/oscar/home/kduru1/kanayo_env/lib64/python3.9/site-packages/sklearn/utils/validation.py:1408: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVC\n",
      "Accuracy: 0.708\n",
      "Precision: 0.708\n",
      "Recall: 1.000\n",
      "F1-score: 0.829\n",
      "MLP\n",
      "Accuracy: 0.750\n",
      "Precision: 0.789\n",
      "Recall: 0.882\n",
      "F1-score: 0.833\n",
      "AdaBoost\n",
      "Accuracy: 0.667\n",
      "Precision: 0.737\n",
      "Recall: 0.824\n",
      "F1-score: 0.778\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/oscar/home/kduru1/kanayo_env/lib64/python3.9/site-packages/sklearn/neighbors/_classification.py:239: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K-Neigh\n",
      "Accuracy: 0.625\n",
      "Precision: 0.900\n",
      "Recall: 0.529\n",
      "F1-score: 0.667\n",
      "BNB\n",
      "Accuracy: 0.708\n",
      "Precision: 0.727\n",
      "Recall: 0.941\n",
      "F1-score: 0.821\n",
      "Logistic R\n",
      "Accuracy: 0.750\n",
      "Precision: 0.762\n",
      "Recall: 0.941\n",
      "F1-score: 0.842\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/oscar/home/kduru1/kanayo_env/lib64/python3.9/site-packages/sklearn/utils/validation.py:1408: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/oscar/home/kduru1/kanayo_env/lib64/python3.9/site-packages/sklearn/utils/validation.py:1408: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost\n",
      "Accuracy: 0.583\n",
      "Precision: 0.667\n",
      "Recall: 0.824\n",
      "F1-score: 0.737\n"
     ]
    }
   ],
   "source": [
    "#SVC\n",
    "svc_model = SVC(kernel='rbf', C=20, gamma='scale', random_state=41, probability = True) \n",
    "svc_model.fit(trainX, trainY)\n",
    "\n",
    "y_pred = svc_model.predict(testX) \n",
    "y_probs = svc_model.predict_proba(testX)[:, 1] \n",
    "\n",
    "\n",
    "accuracy = accuracy_score(testY, y_pred)\n",
    "precision = precision_score(testY, y_pred)\n",
    "recall = recall_score(testY, y_pred)\n",
    "f1 = f1_score(testY, y_pred)\n",
    "\n",
    "print(\"SVC\")\n",
    "print(f\"Accuracy: {accuracy:.3f}\")\n",
    "print(f\"Precision: {precision:.3f}\")\n",
    "print(f\"Recall: {recall:.3f}\")\n",
    "print(f\"F1-score: {f1:.3f}\")\n",
    "\n",
    "aucreport1 = roc_auc_score(testY, y_probs)\n",
    "fpr1, tpr1, _ = roc_curve(testY, y_probs)\n",
    "\n",
    "\n",
    "#MLP\n",
    "mlp_model = MLPClassifier(hidden_layer_sizes=(100,), activation='relu', solver='adam', \n",
    "                          alpha=0.0001, max_iter=500, random_state=42)\n",
    "\n",
    "mlp_model.fit(trainX, trainY)\n",
    "\n",
    "y_pred = mlp_model.predict(testX) \n",
    "y_probs = mlp_model.predict_proba(testX)[:, 1] \n",
    "\n",
    "accuracy = accuracy_score(testY, y_pred)\n",
    "precision = precision_score(testY, y_pred)\n",
    "recall = recall_score(testY, y_pred)\n",
    "f1 = f1_score(testY, y_pred)\n",
    "\n",
    "print(\"MLP\")\n",
    "print(f\"Accuracy: {accuracy:.3f}\")\n",
    "print(f\"Precision: {precision:.3f}\")\n",
    "print(f\"Recall: {recall:.3f}\")\n",
    "print(f\"F1-score: {f1:.3f}\")\n",
    "\n",
    "aucreport2 = roc_auc_score(testY, y_probs) \n",
    "\n",
    "fpr2, tpr2, _ = roc_curve(testY, y_probs)\n",
    "\n",
    "\n",
    "#AdaBoost\n",
    "adaboost_model = AdaBoostClassifier(\n",
    "    n_estimators=50,\n",
    "    learning_rate=1.0,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "adaboost_model.fit(trainX, trainY)\n",
    "\n",
    "y_pred = adaboost_model.predict(testX)\n",
    "y_probs = adaboost_model.predict_proba(testX)[:, 1]\n",
    "\n",
    "accuracy = accuracy_score(testY, y_pred)\n",
    "precision = precision_score(testY, y_pred)\n",
    "recall = recall_score(testY, y_pred)\n",
    "f1 = f1_score(testY, y_pred)\n",
    "\n",
    "print(\"AdaBoost\")\n",
    "print(f\"Accuracy: {accuracy:.3f}\")\n",
    "print(f\"Precision: {precision:.3f}\")\n",
    "print(f\"Recall: {recall:.3f}\")\n",
    "print(f\"F1-score: {f1:.3f}\")\n",
    "\n",
    "aucreport3 = roc_auc_score(testY, y_probs)\n",
    "\n",
    "fpr3, tpr3, _ = roc_curve(testY, y_probs)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#K-Neighbors Classifier\n",
    "model = KNeighborsClassifier(n_neighbors=2, metric='minkowski', p=20)\n",
    "\n",
    "\n",
    "model.fit(trainX, trainY)\n",
    "\n",
    "y_pred = model.predict(testX)\n",
    "y_probs = model.predict_proba(testX)[:, 1]\n",
    "\n",
    "accuracy = accuracy_score(testY, y_pred)\n",
    "precision = precision_score(testY, y_pred)\n",
    "recall = recall_score(testY, y_pred)\n",
    "f1 = f1_score(testY, y_pred)\n",
    "\n",
    "print(\"K-Neigh\")\n",
    "print(f\"Accuracy: {accuracy:.3f}\")\n",
    "print(f\"Precision: {precision:.3f}\")\n",
    "print(f\"Recall: {recall:.3f}\")\n",
    "print(f\"F1-score: {f1:.3f}\")\n",
    "\n",
    "aucreport4 = roc_auc_score(testY, y_probs)\n",
    "fpr4, tpr4, _ = roc_curve(testY, y_probs)\n",
    "\n",
    "\n",
    "\n",
    "#BNB\n",
    "model = BernoulliNB()\n",
    "model.fit(trainX, trainY)\n",
    "\n",
    "y_pred = model.predict(testX)\n",
    "y_probs = model.predict_proba(testX)[:, 1]\n",
    "\n",
    "accuracy = accuracy_score(testY, y_pred)\n",
    "precision = precision_score(testY, y_pred)\n",
    "recall = recall_score(testY, y_pred)\n",
    "f1 = f1_score(testY, y_pred)\n",
    "\n",
    "print(\"BNB\")\n",
    "print(f\"Accuracy: {accuracy:.3f}\")\n",
    "print(f\"Precision: {precision:.3f}\")\n",
    "print(f\"Recall: {recall:.3f}\")\n",
    "print(f\"F1-score: {f1:.3f}\")\n",
    "\n",
    "aucreport5 = roc_auc_score(testY, y_probs)\n",
    "\n",
    "fpr5, tpr5, _ = roc_curve(testY, y_probs)\n",
    "\n",
    "\n",
    "#Logistic Regression\n",
    "model = LogisticRegression()\n",
    "model.fit(trainX, trainY)\n",
    "\n",
    "y_pred = model.predict(testX)\n",
    "y_probs = model.predict_proba(testX)[:, 1]\n",
    "\n",
    "accuracy = accuracy_score(testY, y_pred)\n",
    "precision = precision_score(testY, y_pred)\n",
    "recall = recall_score(testY, y_pred)\n",
    "f1 = f1_score(testY, y_pred)\n",
    "\n",
    "print(\"Logistic R\")\n",
    "print(f\"Accuracy: {accuracy:.3f}\")\n",
    "print(f\"Precision: {precision:.3f}\")\n",
    "print(f\"Recall: {recall:.3f}\")\n",
    "print(f\"F1-score: {f1:.3f}\")\n",
    "\n",
    "aucreport6 = roc_auc_score(testY, y_probs)\n",
    "\n",
    "fpr6, tpr6, _ = roc_curve(testY, y_probs)\n",
    "\n",
    "\n",
    "\n",
    "#XGBoost\n",
    "model = xgb.XGBClassifier(\n",
    "    objective='binary:logistic',\n",
    "    eval_metric='logloss',\n",
    ")\n",
    "\n",
    "model.fit(trainX, trainY)\n",
    "\n",
    "y_pred = model.predict(testX)\n",
    "y_probs = model.predict_proba(testX)[:, 1]\n",
    "\n",
    "accuracy = accuracy_score(testY, y_pred)\n",
    "precision = precision_score(testY, y_pred)\n",
    "recall = recall_score(testY, y_pred)\n",
    "f1 = f1_score(testY, y_pred)\n",
    "\n",
    "print(\"XGBoost\")\n",
    "print(f\"Accuracy: {accuracy:.3f}\")\n",
    "print(f\"Precision: {precision:.3f}\")\n",
    "print(f\"Recall: {recall:.3f}\")\n",
    "print(f\"F1-score: {f1:.3f}\")\n",
    "\n",
    "aucreport7 = roc_auc_score(testY, y_probs)\n",
    "\n",
    "fpr7, tpr7, _ = roc_curve(testY, y_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2bdf3e3-ee6a-4a7f-8624-6694b2088bfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Optimal therapy tool\n",
    "\n",
    "import itertools\n",
    "\n",
    "# Generate all binary combinations of length 5\n",
    "binary_combinations = list(itertools.product([0, 1], repeat=9))\n",
    "\n",
    "# Print all combinations\n",
    "prob_responses = []\n",
    "combos = []\n",
    "max_prob = 0\n",
    "max_combo = []\n",
    "for combo in binary_combinations:\n",
    "    with T.no_grad():\n",
    "        testX[14][0:9] = list(combo)\n",
    "        testX[14][8] = 0\n",
    "        TestX = T.tensor(testX, dtype=T.float32)\n",
    "        outputs = analysis_model(TestX[14])\n",
    "        prob = T.sigmoid(outputs).cpu().numpy().flatten()\n",
    "        if prob > max_prob:\n",
    "            max_prob = prob\n",
    "            max_combo = combo\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
