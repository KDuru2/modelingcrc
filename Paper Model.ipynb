{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f2a7bc78-98fb-4001-b909-af3281c73376",
   "metadata": {},
   "source": [
    "## Data Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b39e510-64e2-421e-87c8-94d05a15a1d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch as T\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import roc_curve, auc, roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.feature_selection import SelectPercentile, mutual_info_classif\n",
    "from sklearn.metrics import precision_recall_curve, f1_score, accuracy_score, precision_score, recall_score\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn import preprocessing\n",
    "import matplotlib.pyplot as plt\n",
    "import shap\n",
    "import ray\n",
    "import xgboost\n",
    "from ray import tune\n",
    "from ray.train import Checkpoint, get_checkpoint\n",
    "from ray.tune.schedulers import ASHAScheduler\n",
    "from ray.tune.search.hyperopt import HyperOptSearch\n",
    "import ray.cloudpickle as pickle\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from hyperopt import hp\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08c0b722-fc29-4f8a-82cc-725107ad1ebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cancer gene list initialization\n",
    "cancerdf = pd.read_csv(\"/gpfs/data/dgamsiz/kduru1/data/known_cancer.txt\", sep =\"\\t\")\n",
    "cancer_genes = list(cancerdf['Symbol'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06b49aa5-b7fe-4409-a69e-afb6fd211a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "#RNA Dataframe initialization\n",
    "rnadf = pd.read_csv(\"/gpfs/data/dgamsiz/kduru1/data/modified_rnaseq_data.txt\", sep =\"\\t\")\n",
    "\n",
    "rnadf = rnadf.transpose()\n",
    "rnadf.drop('Entrez_Gene_Id',inplace=True)\n",
    "rnadf = rnadf.reset_index() \n",
    "rnadf['index'] = rnadf['index'].str.replace('-01', '')\n",
    "rnadf.columns = rnadf.iloc[0]\n",
    "rnadf = rnadf.drop(0)\n",
    "rnadf = rnadf.rename(columns={'Hugo_Symbol': 'PATIENT_ID'})\n",
    "rnadf.columns = rnadf.columns.astype(str)\n",
    "rnadf.set_index('PATIENT_ID', inplace = True)\n",
    "#Removes missing RNA data\n",
    "rnadf.dropna(axis=1, inplace=True)\n",
    "\n",
    "#Averages duplicate RNA data\n",
    "duplicates = rnadf.columns[rnadf.columns.duplicated()]\n",
    "\n",
    "for duplicate in duplicates:\n",
    "    avg = rnadf[duplicate].mean(axis=1)\n",
    "    rnadf['avg_' + duplicate] = avg\n",
    "\n",
    "rnadf.drop(duplicates, axis=1, inplace=True)\n",
    "\n",
    "rnadf = rnadf.add_prefix('rna_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bdbcac2-dbd0-44fd-b981-ed908705ac5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Mutation DataFrame Initialization\n",
    "mutdf = pd.read_csv(\"/gpfs/data/dgamsiz/kduru1/data/data_mutations.txt\", sep =\"\\t\")\n",
    "\n",
    "columns_to_keep = ['Hugo_Symbol', 'Variant_Classification', 'Tumor_Sample_Barcode']\n",
    "mutdf = mutdf[columns_to_keep]\n",
    "mutdf = mutdf.set_index('Tumor_Sample_Barcode')\n",
    "\n",
    "mutdf = mutdf[~mutdf['Variant_Classification'].isin([\"Silent\",\"3'Flank\",\"3'UTR\",\"5'Flank\",\"5'UTR\",\"Intron\"])]\n",
    "mutdf.drop('Variant_Classification', axis=1, inplace=True)\n",
    "mutdf.index = [s.strip('-01') for s in mutdf.index]\n",
    "mutdf.index.names = ['PATIENT_ID']\n",
    "mutdf = pd.get_dummies(mutdf, columns=['Hugo_Symbol'])\n",
    "\n",
    "mutdf = mutdf.groupby(\"PATIENT_ID\").max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec77ce1a-56a7-4bc4-9977-609e319ba0da",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialize Clinical Dataframe and combine with Treatment Dataframe\n",
    "clindf = pd.read_csv(\"/gpfs/data/dgamsiz/kduru1/data/data_clinical_patient.txt\", sep =\"\\t\")\n",
    "treatmentdf = pd.read_csv(\"/gpfs/data/dgamsiz/kduru1/data/data_timeline_treatment.txt\", sep =\"\\t\")\n",
    "statdf = pd.read_csv(\"/gpfs/data/dgamsiz/kduru1/data/data_timeline_status.txt\", sep =\"\\t\")\n",
    "\n",
    "clindf.set_index('PATIENT_ID', inplace=True)\n",
    "statdf.set_index('PATIENT_ID', inplace=True)\n",
    "statusdf = statdf['PRIMARY_THERAPY_OUTCOME_SUCCESS'].dropna()\n",
    "\n",
    "\n",
    "for col in treatmentdf.columns:\n",
    "    if col != 'AGENT' and col != 'MEASURE_OF_RESPONSE' and col != 'PATIENT_ID' and col != 'START_DATE':\n",
    "        treatmentdf.drop(col, axis=1, inplace=True)\n",
    "\n",
    "#Transform data into binary format - 0.5\n",
    "treatmentdf.replace(to_replace='clinical progressive disease', value=False, inplace=True)\n",
    "treatmentdf.replace(to_replace='stable progressive disease', value=False, inplace=True)\n",
    "treatmentdf.replace(to_replace='radiographic progressive disease', value=False, inplace=True)\n",
    "treatmentdf.replace(to_replace='stable disease', value=False, inplace=True)\n",
    "treatmentdf.replace(to_replace='partial response', value=True, inplace=True)\n",
    "treatmentdf.replace(to_replace='complete response', value=True, inplace=True)\n",
    "statusdf.replace(to_replace='Complete Remission/Response', value=True, inplace=True)\n",
    "statusdf.replace(to_replace='Partial Remission/Response', value=True, inplace=True)\n",
    "statusdf.replace(to_replace='Stable Disease', value=False, inplace=True)\n",
    "statusdf.replace(to_replace='Progressive Disease', value=False, inplace=True)\n",
    "clindf.replace(to_replace='Male', value= 1, inplace=True)\n",
    "clindf.replace(to_replace='Female', value= 0, inplace=True)\n",
    "\n",
    "#Merge treatmentdf with clin params of interest\n",
    "paramsetup = {\n",
    "    'SEX' : clindf['SEX'],\n",
    "    'ANCES' : clindf['GENETIC_ANCESTRY_LABEL'],\n",
    "    'AGE' : clindf['AGE']\n",
    "}\n",
    "\n",
    "clinparameters = pd.DataFrame(paramsetup)\n",
    "idparamdf = treatmentdf.merge(clinparameters, on='PATIENT_ID')\n",
    "idparamdf.set_index('PATIENT_ID', inplace=True)\n",
    "\n",
    "\n",
    "#Augments treatment database with overall status database\n",
    "nans = idparamdf['MEASURE_OF_RESPONSE'].isna()\n",
    "for pt in idparamdf.index:\n",
    "    if nans[pt].all() and pt in statusdf.index:\n",
    "        last_med = np.max(idparamdf.loc[pt,'START_DATE'])\n",
    "        idparamdf.loc[(idparamdf['START_DATE'] == last_med) & (idparamdf.index == pt), 'MEASURE_OF_RESPONSE'] = statusdf[pt]\n",
    "\n",
    "idparamdf.replace(' ', np.nan, inplace=True)\n",
    "idparamdf.dropna(axis=0, inplace=True)\n",
    "\n",
    "#Remove pts missing clinical data\n",
    "idparamdf.replace(' ', np.nan, inplace=True)\n",
    "idparamdf.dropna(axis=0, inplace=True)\n",
    "\n",
    "#One-hot encoding for categorical data\n",
    "idparamdf = pd.get_dummies(idparamdf, columns=['AGENT'])\n",
    "\n",
    "\n",
    "non_med = ['AGENT_Radiation 1', 'AGENT_Nos', 'AGENT_Radiation 2']\n",
    "\n",
    "#if non_med in idparamdf.columns:\n",
    "idparamdf.drop(non_med, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "517a973a-0b77-421b-a32e-4aacb5c34091",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Combines concurrent regiments into one entry based on start date\n",
    "agents = list()\n",
    "agents.append('MEASURE_OF_RESPONSE')\n",
    "\n",
    "for item in list(idparamdf.columns):\n",
    "    if 'AGENT' in item:\n",
    "        agents.append(item)\n",
    "\n",
    "clindf = pd.DataFrame()\n",
    "\n",
    "for pt in idparamdf.index.unique():\n",
    "    df = idparamdf.loc[pt]\n",
    "    starts = np.sort(np.unique(df['START_DATE']))\n",
    "    if isinstance(df, pd.DataFrame):\n",
    "        startdf = df[(df['START_DATE'] == starts[0])]\n",
    "        startdf = startdf[agents]\n",
    "        boolsum = startdf.apply(lambda row: any(row), axis=0)\n",
    "        new_row = pd.DataFrame(boolsum).transpose()\n",
    "        new_row.index = [pt]\n",
    "        clindf = pd.concat([clindf, new_row])\n",
    "    else:\n",
    "        df = df[agents]\n",
    "        new_row = pd.DataFrame(df).transpose()\n",
    "        clindf = pd.concat([clindf, new_row])\n",
    "\n",
    "\n",
    "agents.remove('MEASURE_OF_RESPONSE')\n",
    "valid_single_agents = [x.replace('AGENT_','') for x in agents if \"+\" not in x]\n",
    "double_agents = [x for x in agents if \"+\" in x]\n",
    "\n",
    "#Handle '+' medication entries\n",
    "for row in range(len(clindf)):\n",
    "    for col in clindf.columns:\n",
    "        if '+' in col and clindf.iloc[row][col]:\n",
    "            single_agents = col.replace('AGENT_','').split(' + ')\n",
    "            for agent in single_agents:\n",
    "                if agent in valid_single_agents:\n",
    "                    addagent = 'AGENT_' + agent\n",
    "                    clindf.iloc[row][addagent] = True\n",
    "\n",
    "clindf.drop(columns=double_agents, inplace=True)\n",
    "\n",
    "#Add back in ages, sex, MOR, ancestry\n",
    "ages = idparamdf['AGE'].loc[~idparamdf['AGE'].index.duplicated(keep='first')]\n",
    "sex = idparamdf['SEX'].loc[~idparamdf['SEX'].index.duplicated(keep='first')]\n",
    "ances = idparamdf['ANCES'].loc[~idparamdf['ANCES'].index.duplicated(keep='first')]\n",
    "\n",
    "clindf = clindf.join(ages)\n",
    "clindf = clindf.join(sex)\n",
    "clindf = clindf.join(ances)\n",
    "\n",
    "clindf = pd.get_dummies(clindf, columns=['ANCES'])\n",
    "clindf.index.name = 'PATIENT_ID'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1053220-c578-4d32-8234-8d4350acd933",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Merge dfs\n",
    "halfmergedf = clindf.join(mutdf, how=\"inner\")\n",
    "mergedf = halfmergedf.join(rnadf, how=\"inner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41acd3e9-7c5e-4d10-8a65-9353b8a88034",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Shuffles dataframe with reproducible seed\n",
    "ids = mergedf.index.unique().tolist()\n",
    "random.seed(23)\n",
    "random.shuffle(ids)\n",
    "shuffledf = mergedf.reset_index()\n",
    "shuffledf = shuffledf.set_index('PATIENT_ID').loc[ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c72063ff-858d-48ca-9da0-8f93f8c9722b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop constant values\n",
    "nonconstantdf = shuffledf.loc[:, shuffledf.nunique() != 1]\n",
    "\n",
    "#Convert logical values to ints\n",
    "nonconstantdf.replace(to_replace=False, value=0 , inplace=True)\n",
    "nonconstantdf.replace(to_replace=True, value=1, inplace = True)\n",
    "nonconstantdf= nonconstantdf.astype(float)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f2c3960-bf4d-4813-ae2e-a83abdca43b7",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36db5600-3ed2-483e-b272-e955adb98d3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split into train (70%) and test (30%) with stratification\n",
    "train_df, test_df = train_test_split(nonconstantdf, test_size=0.3, random_state=41, stratify=nonconstantdf['MEASURE_OF_RESPONSE'])\n",
    "\n",
    "# Split test into test (80%) and validation (20%) with stratification\n",
    "test_df, valid_df = train_test_split(test_df, test_size=0.2, random_state=41, stratify=test_df['MEASURE_OF_RESPONSE'])\n",
    "\n",
    "# Define target (y) and features (X)\n",
    "y_train = train_df.iloc[:, 0]\n",
    "df_train = train_df.iloc[:, 1:]\n",
    "\n",
    "y_test = test_df.iloc[:, 0]\n",
    "df_test = test_df.iloc[:, 1:]\n",
    "\n",
    "y_valid = valid_df.iloc[:, 0]\n",
    "df_valid = valid_df.iloc[:, 1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f726e3af-78fc-49d3-b4c4-1606730ffa21",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data Splitting\n",
    "rnadf_train = df_train.filter(like='rna_')\n",
    "mutdf_train = df_train.filter(like='Hugo_')\n",
    "clindf_train = df_train.drop(columns=rnadf_train.columns.union(mutdf_train.columns))\n",
    "\n",
    "rnadf_test = df_test.filter(like='rna_')\n",
    "mutdf_test = df_test.filter(like='Hugo_')\n",
    "clindf_test = df_test.drop(columns=rnadf_test.columns.union(mutdf_test.columns))\n",
    "\n",
    "rnadf_valid = df_valid.filter(like='rna_')\n",
    "mutdf_valid = df_valid.filter(like='Hugo_')\n",
    "clindf_valid = df_valid.drop(columns=rnadf_valid.columns.union(mutdf_valid.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d5da80f-f6c4-44b6-aa3a-13abd729b5f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#RNA Normalization\n",
    "scaler = preprocessing.Normalizer()\n",
    "scaler.fit(rnadf_train)\n",
    "\n",
    "srnadf_train = pd.DataFrame(scaler.transform(rnadf_train), \n",
    "                        columns=rnadf_train.columns, index=rnadf_train.index)\n",
    "\n",
    "srnadf_test = pd.DataFrame(scaler.transform(rnadf_test), \n",
    "                        columns=rnadf_test.columns, index=rnadf_test.index)\n",
    "\n",
    "srnadf_valid = pd.DataFrame(scaler.transform(rnadf_valid), \n",
    "                        columns=rnadf_valid.columns, index=rnadf_valid.index)\n",
    "\n",
    "srna_np = rnadf_train.to_numpy()\n",
    "corr_matrix = np.corrcoef(srna_np, rowvar=False)\n",
    "corr_triangle = np.triu(corr_matrix, k = 1)\n",
    "feature_var = rnadf_train.var()\n",
    "\n",
    "#Find associated features and remove\n",
    "to_remove = set()\n",
    "corr_threshold = 0.80\n",
    "c = 0\n",
    "v = 0\n",
    "r = 0\n",
    "\n",
    "corr_ind1, corr_ind2 = np.where(corr_triangle >= corr_threshold)\n",
    "\n",
    "#Which feature is known for cancer? Which feature has higher variance?\n",
    "for index in range(len(corr_ind1)):\n",
    "    corr_feature1 = srnadf_train.columns[corr_ind1[index]]\n",
    "    corr_feature2 = srnadf_train.columns[corr_ind2[index]]\n",
    "\n",
    "    if corr_feature1.strip('rna_') in cancer_genes and corr_feature2.strip('rna_') not in cancer_genes:\n",
    "        to_remove.add(corr_feature1)\n",
    "        c =  c+1\n",
    "\n",
    "    elif corr_feature2.strip('rna_') in cancer_genes and corr_feature1.strip('rna_') not in cancer_genes:\n",
    "        to_remove.add(corr_feature2)\n",
    "        c= c+1\n",
    "\n",
    "    elif feature_var[corr_feature1] > feature_var[corr_feature2]:\n",
    "        to_remove.add(corr_feature2)\n",
    "        v= v+1\n",
    "\n",
    "    elif feature_var[corr_feature2] > feature_var[corr_feature1]:\n",
    "        to_remove.add(corr_feature1)\n",
    "        v = v+1\n",
    "\n",
    "\n",
    "urnadf_train = rnadf_train.drop(columns = to_remove)\n",
    "urnadf_test = rnadf_test.drop(columns = to_remove)\n",
    "urnadf_valid = rnadf_valid.drop(columns = to_remove)\n",
    "\n",
    "\n",
    "print(f\"We adjudicated {c} features based on known cancer and {v} features by variance and {r} features randomly.\")\n",
    "print(f\"We removed {len(to_remove)} total features.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eec9dc2-56da-475f-8235-4b5ae64218aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "preselector = SelectPercentile(mutual_info_classif, percentile=2)\n",
    "preselector.fit(urnadf_train, y_train)\n",
    "\n",
    "selected_features = preselector.get_support(indices=True) \n",
    "psrnadf_train = urnadf_train.iloc[:, selected_features]\n",
    "psrnadf_test = urnadf_test.iloc[:, selected_features]\n",
    "psrnadf_valid = urnadf_valid.iloc[:, selected_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddeb15cf-6143-48be-92a3-2b2de5ba9afa",
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc = RandomForestClassifier(n_estimators=50, random_state=26, n_jobs=-1)\n",
    "selector = RFECV(rfc, min_features_to_select=200, step=5, cv=5, n_jobs=-1)\n",
    "selector.fit(psrnadf_train, y_train)\n",
    "\n",
    "selected_features = selector.get_support(indices=True) \n",
    "rfrnadf_train = psrnadf_train.iloc[:, selected_features]\n",
    "rfrnadf_test = psrnadf_test.iloc[:, selected_features]\n",
    "rfrnadf_valid = psrnadf_valid.iloc[:, selected_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9529d83-8919-451c-9173-964a39009df0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Mutation Data Processing\n",
    "constant_cols = mutdf_train.loc[:, mutdf_train.nunique() == 1]\n",
    "\n",
    "vmutdf_train = mutdf_train.drop(columns=constant_cols)\n",
    "vmutdf_test = mutdf_test.drop(columns=constant_cols)\n",
    "vmutdf_valid = mutdf_valid.drop(columns=constant_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2e63a56-1fd2-413d-ba6e-a2c7b4b8e00f",
   "metadata": {},
   "outputs": [],
   "source": [
    "mut_np = vmutdf_train.to_numpy()\n",
    "corr_matrix = np.corrcoef(mut_np, rowvar=False)\n",
    "corr_triangle = np.triu(corr_matrix, k = 1)\n",
    "feature_var = vmutdf_train.var()\n",
    "\n",
    "#Find associated features and remove\n",
    "to_remove = set()\n",
    "corr_threshold = 0.80\n",
    "c = 0\n",
    "v = 0\n",
    "r = 0\n",
    "\n",
    "corr_ind1, corr_ind2 = np.where(corr_triangle >= corr_threshold)\n",
    "\n",
    "#Which feature is known for cancer? Which feature has higher variance?\n",
    "for index in range(len(corr_ind1)):\n",
    "    corr_feature1 = vmutdf_train.columns[corr_ind1[index]]\n",
    "    corr_feature2 = vmutdf_train.columns[corr_ind2[index]]\n",
    "\n",
    "    if corr_feature1.strip('Hugo_Symbol_') in cancer_genes and corr_feature2.strip('Hugo_Symbol_') not in cancer_genes:\n",
    "        to_remove.add(corr_feature1)\n",
    "        c =  c+1\n",
    "\n",
    "    elif corr_feature2.strip('Hugo_Symbol_') in cancer_genes and corr_feature1.strip('Hugo_Symbol_') not in cancer_genes:\n",
    "        to_remove.add(corr_feature2)\n",
    "        c= c+1\n",
    "\n",
    "    elif feature_var[corr_feature1] > feature_var[corr_feature2]:\n",
    "        to_remove.add(corr_feature2)\n",
    "        v= v+1\n",
    "\n",
    "    elif feature_var[corr_feature2] > feature_var[corr_feature1]:\n",
    "        to_remove.add(corr_feature1)\n",
    "        v = v+1\n",
    "\n",
    "\n",
    "umutdf_train = vmutdf_train.drop(columns = to_remove)\n",
    "umutdf_test = vmutdf_test.drop(columns = to_remove)\n",
    "umutdf_valid = vmutdf_valid.drop(columns = to_remove)\n",
    "\n",
    "\n",
    "print(f\"We adjudicated {c} features based on known cancer and {v} features by variance and {r} features randomly.\")\n",
    "print(f\"We removed {len(to_remove)} total features.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c738d72c-02ba-4a58-88c8-83dfd6aac1a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "preselector = SelectPercentile(mutual_info_classif, percentile=3)\n",
    "preselector.fit(umutdf_train, y_train)\n",
    "\n",
    "selected_features = preselector.get_support(indices=True) \n",
    "psmutdf_train = umutdf_train.iloc[:, selected_features]\n",
    "psmutdf_test = umutdf_test.iloc[:, selected_features]\n",
    "psmutdf_valid = umutdf_valid.iloc[:, selected_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "303b1b0e-15fc-48c7-bda0-e92f1b957429",
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc = RandomForestClassifier(n_estimators=50, random_state=26, n_jobs=-1)\n",
    "selector = RFECV(rfc, min_features_to_select=200, step=5, cv=5, n_jobs=-1)\n",
    "selector.fit(psmutdf_train, y_train)\n",
    "\n",
    "selected_features = selector.get_support(indices=True) \n",
    "rfmutdf_train = psmutdf_train.iloc[:, selected_features]\n",
    "rfmutdf_test = psmutdf_test.iloc[:, selected_features]\n",
    "rfmutdf_valid = psmutdf_valid.iloc[:, selected_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc391caa-a0a8-422d-92f1-8d7d278488a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Recombine categorical and numeric data\n",
    "mclindf_train = clindf_train.reset_index().drop('PATIENT_ID', axis = 1)\n",
    "mrnadf_train = rfrnadf_train.reset_index().drop('PATIENT_ID', axis = 1)\n",
    "mmutdf_train = rfmutdf_train.reset_index().drop('PATIENT_ID', axis = 1)\n",
    "halfmergedf_train = mclindf_train.join(mrnadf_train, how=\"inner\")\n",
    "mergedf_train = halfmergedf_train.join(mmutdf_train, how=\"inner\")\n",
    "\n",
    "mclindf_test = clindf_test.reset_index().drop('PATIENT_ID', axis = 1)\n",
    "mrnadf_test = rfrnadf_test.reset_index().drop('PATIENT_ID', axis = 1)\n",
    "mmutdf_test = rfmutdf_test.reset_index().drop('PATIENT_ID', axis = 1)\n",
    "halfmergedf_test = mclindf_test.join(mrnadf_test, how=\"inner\")\n",
    "mergedf_test = halfmergedf_test.join(mmutdf_test, how=\"inner\")\n",
    "\n",
    "mclindf_valid = clindf_valid.reset_index().drop('PATIENT_ID', axis = 1)\n",
    "mrnadf_valid = rfrnadf_valid.reset_index().drop('PATIENT_ID', axis = 1)\n",
    "mmutdf_valid = rfmutdf_valid.reset_index().drop('PATIENT_ID', axis = 1)\n",
    "halfmergedf_valid = mclindf_valid.join(mrnadf_valid, how=\"inner\")\n",
    "mergedf_valid = halfmergedf_valid.join(mmutdf_valid, how=\"inner\")\n",
    "\n",
    "mergedf_train.to_csv('2ntrainX.csv', index=False)\n",
    "mergedf_test.to_csv('2ntestX.csv', index=False)\n",
    "mergedf_valid.to_csv('2nvalidX.csv', index=False)\n",
    "y_train.to_csv('2ntrainY.csv', index=False)\n",
    "y_test.to_csv('2ntestY.csv', index=False)\n",
    "y_valid.to_csv('2nvalidY.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd6f04b4-077e-4844-b84f-425110cc9140",
   "metadata": {},
   "source": [
    "## Final Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a65eaed6-875b-49d0-bf0a-d7937b257bc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_weights(m):\n",
    "    for layer in m.children():\n",
    "        if hasattr(layer, 'reset_parameters'):\n",
    "            layer.reset_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ce56775-219a-4b8d-8f79-3909aca228b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loads saved data\n",
    "trainXdf = pd.read_csv('2ntrainX.csv')\n",
    "testXdf = pd.read_csv('2ntestX.csv')\n",
    "validXdf = pd.read_csv('2nvalidX.csv')\n",
    "trainYdf = pd.read_csv('2ntrainY.csv')\n",
    "testYdf = pd.read_csv('2ntestY.csv')\n",
    "validYdf = pd.read_csv('2nvalidY.csv')\n",
    "\n",
    "#Creates tensors\n",
    "trainX = trainXdf.to_numpy()\n",
    "testX = testXdf.to_numpy()\n",
    "validX = validXdf.to_numpy()\n",
    "\n",
    "trainY = trainYdf.to_numpy()\n",
    "testY = testYdf.to_numpy()\n",
    "validY = validYdf.to_numpy()\n",
    "\n",
    "TrainX = T.tensor(trainX, dtype=T.float32)\n",
    "TrainY = T.tensor(trainY, dtype=T.float32).reshape(-1, 1)\n",
    "\n",
    "TestX = T.tensor(testX, dtype=T.float32)\n",
    "TestY = T.tensor(testY, dtype=T.float32).reshape(-1, 1)\n",
    "\n",
    "ValidX = T.tensor(validX, dtype=T.float32)\n",
    "ValidY = T.tensor(validY, dtype=T.float32).reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "179523d2-6e49-465b-9d4e-a87e92ac7db5",
   "metadata": {},
   "outputs": [],
   "source": [
    "paramcount = len(TestX[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86ef4ac0-a274-480a-96b6-725dcfb662cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "nr = sum(trainY == 0)\n",
    "r = sum(trainY == 1)\n",
    "bal = nr/r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b8bd65f-2307-4fab-ab57-960e7a222917",
   "metadata": {},
   "outputs": [],
   "source": [
    "weight = T.tensor([bal], dtype=T.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a728132d-fb14-4ac1-b749-23d444d68680",
   "metadata": {},
   "outputs": [],
   "source": [
    "hparams = {\n",
    "    \"lr\" : hp.uniform('lr', 0.0001, 0.0009),\n",
    "    \"batch\": hp.randint('batch',15,30),\n",
    "    \"decay\" : hp.uniform('decay',0.001, 0.009),\n",
    "    \"do\" : hp.choice('do',[0.5,0.6,0.7, 0.8, 0.9]),\n",
    "    \n",
    "}\n",
    "\n",
    "hyperopt_search = HyperOptSearch(hparams, metric=\"loss\", mode=\"min\")\n",
    "\n",
    "def createNet(do):\n",
    "    model = nn.Sequential(\n",
    "    nn.Linear(paramcount, 1024),\n",
    "    nn.LeakyReLU(),\n",
    "    nn.Dropout(do),\n",
    "    nn.Linear(1024, 512),\n",
    "    nn.LeakyReLU(),\n",
    "    nn.Dropout(do),\n",
    "    nn.Linear(512, 256),\n",
    "    nn.LeakyReLU(),\n",
    "    nn.Linear(256,1)\n",
    ")\n",
    "    return model\n",
    "\n",
    "def train(hparams, checkpoint_dir=None):\n",
    "    model = createNet(hparams[\"do\"])\n",
    "    reset_weights(model)\n",
    "    lossfxn = nn.BCEWithLogitsLoss(pos_weight = weight)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=hparams[\"lr\"], weight_decay = hparams[\"decay\"])\n",
    "\n",
    "    numepochs = 30\n",
    "    batchsize = hparams[\"batch\"]\n",
    "    for epoch in range(numepochs):\n",
    "        for i in range (0, len(TrainX), batchsize):\n",
    "            Xbatch = TrainX[i:i+batchsize]\n",
    "            ybatch = TrainY[i:i+batchsize]\n",
    "            ypredicted = model(Xbatch)\n",
    "            loss = lossfxn(ypredicted, ybatch)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # Calculate average validation loss\n",
    "        model.eval() \n",
    "        val_loss = 0.0\n",
    "        with T.no_grad():\n",
    "            y_prob = model(TestX).numpy()\n",
    "            for i in range(0, len(validX), batchsize):\n",
    "                Xbatch_val = ValidX[i:i+batchsize]\n",
    "                ybatch_val = ValidY[i:i+batchsize]\n",
    "                ypredicted_val = model(Xbatch_val)\n",
    "                val_loss_batch = lossfxn(ypredicted_val, ybatch_val)\n",
    "                val_loss += val_loss_batch.item()\n",
    "        val_loss /= (len(ValidX) / batchsize)\n",
    "        \n",
    "        # Calculate FPR, TPR, and thresholds\n",
    "        fpr, tpr, thresholds = roc_curve(TestY, y_prob)\n",
    "    \n",
    "        # Calculate the AUC (Area Under the Curve)\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "    \n",
    "        model.train()\n",
    "\n",
    "        ray.train.report(\n",
    "            {\"loss\" : val_loss,\n",
    "            \"AUC\" : roc_auc})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e641bbe-454d-4290-b87b-62a7021c78b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hyperparameter training\n",
    "analysis = tune.run(\n",
    "    train,\n",
    "    search_alg=hyperopt_search,\n",
    "    num_samples = 500,\n",
    "    max_concurrent_trials = 30,\n",
    "    scheduler=ASHAScheduler(\n",
    "        metric = \"loss\",\n",
    "        mode = \"min\",\n",
    "        max_t = 30,\n",
    "        grace_period = 10,\n",
    "        reduction_factor = 2),\n",
    "    resources_per_trial={\"cpu\": 10, \"gpu\": 0},\n",
    "    verbose = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e23a548-7676-4fa6-97d5-ceb2f31dd4ec",
   "metadata": {},
   "source": [
    "## Comparative Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bd5f4e3-6581-4875-81da-bdff1c19c7c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "svc_model = SVC(kernel='rbf', C=20, gamma='scale', random_state=41, probability = True) \n",
    "# Train model\n",
    "svc_model.fit(trainX, trainY)\n",
    "\n",
    "y_pred = svc_model.predict(testX) \n",
    "y_probs = svc_model.predict_proba(testX)[:, 1] \n",
    "\n",
    "\n",
    "accuracy = accuracy_score(testY, y_pred)\n",
    "precision = precision_score(testY, y_pred)\n",
    "recall = recall_score(testY, y_pred)\n",
    "f1 = f1_score(testY, y_pred)\n",
    "\n",
    "print(\"SVC\")\n",
    "print(f\"Accuracy: {accuracy:.3f}\")\n",
    "print(f\"Precision: {precision:.3f}\")\n",
    "print(f\"Recall: {recall:.3f}\")\n",
    "print(f\"F1-score: {f1:.3f}\")\n",
    "\n",
    "aucreport1 = roc_auc_score(testY, y_probs)\n",
    "\n",
    "\n",
    "# Compute ROC curve\n",
    "fpr1, tpr1, _ = roc_curve(testY, y_probs)\n",
    "\n",
    "\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "mlp_model = MLPClassifier(hidden_layer_sizes=(100,), activation='relu', solver='adam', \n",
    "                          alpha=0.0001, max_iter=500, random_state=42)\n",
    "\n",
    "# Train model\n",
    "mlp_model.fit(trainX, trainY)\n",
    "\n",
    "y_pred = mlp_model.predict(testX) \n",
    "y_probs = mlp_model.predict_proba(testX)[:, 1] \n",
    "\n",
    "accuracy = accuracy_score(testY, y_pred)\n",
    "precision = precision_score(testY, y_pred)\n",
    "recall = recall_score(testY, y_pred)\n",
    "f1 = f1_score(testY, y_pred)\n",
    "\n",
    "print(\"MLP\")\n",
    "print(f\"Accuracy: {accuracy:.3f}\")\n",
    "print(f\"Precision: {precision:.3f}\")\n",
    "print(f\"Recall: {recall:.3f}\")\n",
    "print(f\"F1-score: {f1:.3f}\")\n",
    "\n",
    "aucreport2 = roc_auc_score(testY, y_probs) \n",
    "\n",
    "#print(f\"AUC Score: {aucreport:.3f}\")\n",
    "\n",
    "# Compute ROC curve\n",
    "fpr2, tpr2, _ = roc_curve(testY, y_probs)\n",
    "\n",
    "\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "adaboost_model = AdaBoostClassifier(\n",
    "    n_estimators=50,\n",
    "    learning_rate=1.0,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Train model\n",
    "adaboost_model.fit(trainX, trainY)\n",
    "\n",
    "y_pred = adaboost_model.predict(testX)\n",
    "y_probs = adaboost_model.predict_proba(testX)[:, 1]\n",
    "\n",
    "accuracy = accuracy_score(testY, y_pred)\n",
    "precision = precision_score(testY, y_pred)\n",
    "recall = recall_score(testY, y_pred)\n",
    "f1 = f1_score(testY, y_pred)\n",
    "\n",
    "print(\"AdaBoost\")\n",
    "print(f\"Accuracy: {accuracy:.3f}\")\n",
    "print(f\"Precision: {precision:.3f}\")\n",
    "print(f\"Recall: {recall:.3f}\")\n",
    "print(f\"F1-score: {f1:.3f}\")\n",
    "\n",
    "aucreport3 = roc_auc_score(testY, y_probs)\n",
    "\n",
    "#print(f\"AUC Score: {aucreport:.3f}\")\n",
    "\n",
    "# Compute ROC curve\n",
    "fpr3, tpr3, _ = roc_curve(testY, y_probs)\n",
    "\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "# Create K-Neighbors Classifier\n",
    "model = KNeighborsClassifier(n_neighbors=2, metric='minkowski', p=20)\n",
    "\n",
    "# Train model\n",
    "model.fit(trainX, trainY)\n",
    "\n",
    "# Predictions\n",
    "y_pred = model.predict(testX)\n",
    "y_probs = model.predict_proba(testX)[:, 1]\n",
    "\n",
    "accuracy = accuracy_score(testY, y_pred)\n",
    "precision = precision_score(testY, y_pred)\n",
    "recall = recall_score(testY, y_pred)\n",
    "f1 = f1_score(testY, y_pred)\n",
    "\n",
    "print(\"K-Neigh\")\n",
    "print(f\"Accuracy: {accuracy:.3f}\")\n",
    "print(f\"Precision: {precision:.3f}\")\n",
    "print(f\"Recall: {recall:.3f}\")\n",
    "print(f\"F1-score: {f1:.3f}\")\n",
    "\n",
    "aucreport4 = roc_auc_score(testY, y_probs)\n",
    "\n",
    "#print(f\"AUC Score: {aucreport:.3f}\")\n",
    "\n",
    "# Compute ROC curve\n",
    "fpr4, tpr4, _ = roc_curve(testY, y_probs)\n",
    "\n",
    "\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "\n",
    "model = BernoulliNB()\n",
    "# Train model\n",
    "model.fit(trainX, trainY)\n",
    "\n",
    "y_pred = model.predict(testX)\n",
    "y_probs = model.predict_proba(testX)[:, 1]\n",
    "\n",
    "accuracy = accuracy_score(testY, y_pred)\n",
    "precision = precision_score(testY, y_pred)\n",
    "recall = recall_score(testY, y_pred)\n",
    "f1 = f1_score(testY, y_pred)\n",
    "\n",
    "print(\"BNB\")\n",
    "print(f\"Accuracy: {accuracy:.3f}\")\n",
    "print(f\"Precision: {precision:.3f}\")\n",
    "print(f\"Recall: {recall:.3f}\")\n",
    "print(f\"F1-score: {f1:.3f}\")\n",
    "\n",
    "aucreport5 = roc_auc_score(testY, y_probs)\n",
    "\n",
    "#print(f\"AUC Score: {aucreport:.3f}\")\n",
    "\n",
    "# Compute ROC curve\n",
    "fpr5, tpr5, _ = roc_curve(testY, y_probs)\n",
    "\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# Train logistic regression model\n",
    "model = LogisticRegression()\n",
    "model.fit(trainX, trainY)\n",
    "\n",
    "y_pred = model.predict(testX)\n",
    "y_probs = model.predict_proba(testX)[:, 1]\n",
    "\n",
    "accuracy = accuracy_score(testY, y_pred)\n",
    "precision = precision_score(testY, y_pred)\n",
    "recall = recall_score(testY, y_pred)\n",
    "f1 = f1_score(testY, y_pred)\n",
    "\n",
    "print(\"Logistic R\")\n",
    "print(f\"Accuracy: {accuracy:.3f}\")\n",
    "print(f\"Precision: {precision:.3f}\")\n",
    "print(f\"Recall: {recall:.3f}\")\n",
    "print(f\"F1-score: {f1:.3f}\")\n",
    "\n",
    "aucreport6 = roc_auc_score(testY, y_probs)\n",
    "\n",
    "#print(f\"AUC Score: {aucreport:.3f}\")\n",
    "\n",
    "# Compute ROC curve\n",
    "fpr6, tpr6, _ = roc_curve(testY, y_probs)\n",
    "\n",
    "\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "\n",
    "\n",
    "# Define XGBoost classifier\n",
    "model = xgb.XGBClassifier(\n",
    "    objective='binary:logistic',\n",
    "    eval_metric='logloss',\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "model.fit(trainX, trainY)\n",
    "\n",
    "y_pred = model.predict(testX)\n",
    "y_probs = model.predict_proba(testX)[:, 1]\n",
    "\n",
    "accuracy = accuracy_score(testY, y_pred)\n",
    "precision = precision_score(testY, y_pred)\n",
    "recall = recall_score(testY, y_pred)\n",
    "f1 = f1_score(testY, y_pred)\n",
    "\n",
    "print(\"XGBoost\")\n",
    "print(f\"Accuracy: {accuracy:.3f}\")\n",
    "print(f\"Precision: {precision:.3f}\")\n",
    "print(f\"Recall: {recall:.3f}\")\n",
    "print(f\"F1-score: {f1:.3f}\")\n",
    "\n",
    "aucreport7 = roc_auc_score(testY, y_probs)\n",
    "\n",
    "#print(f\"AUC Score: {aucreport:.3f}\")\n",
    "\n",
    "# Compute ROC curve\n",
    "fpr7, tpr7, _ = roc_curve(testY, y_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2bdf3e3-ee6a-4a7f-8624-6694b2088bfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Optimal therapy tool\n",
    "\n",
    "import itertools\n",
    "\n",
    "# Generate all binary combinations of length 5\n",
    "binary_combinations = list(itertools.product([0, 1], repeat=9))\n",
    "\n",
    "# Print all combinations\n",
    "prob_responses = []\n",
    "combos = []\n",
    "max_prob = 0\n",
    "max_combo = []\n",
    "for combo in binary_combinations:\n",
    "    with T.no_grad():\n",
    "        testX[14][0:9] = list(combo)\n",
    "        testX[14][8] = 0\n",
    "        TestX = T.tensor(testX, dtype=T.float32)\n",
    "        outputs = analysis_model(TestX[14])\n",
    "        prob = T.sigmoid(outputs).cpu().numpy().flatten()\n",
    "        if prob > max_prob:\n",
    "            max_prob = prob\n",
    "            max_combo = combo\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
